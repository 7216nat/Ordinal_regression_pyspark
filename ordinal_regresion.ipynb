{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal Ordinal\n",
    "\n",
    "### Xukai Chen & Julio \n",
    "### Curso: TDM - 2022/2023\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para  poder entender el concepto esta regresión y el problema a resolver, debemos recordar y recurrir a los conceptos basicos del data science, las variables.\n",
    "\n",
    "Como ya sabemos, tanto regresión como clasificación, son un modelo de estimacion que tienen como objetivo predecir una relacion de dependencia entre una variable independiente  *Y* y una o varias variables dependientes  *X*.\n",
    "\n",
    "<img src=\"./images/regr_cfc.png\" alt=\"regre_cfc.png\" width=\"600\"/>\n",
    "\n",
    "#### Los ejemplos más destacados de ambos tipos de algoritmos \n",
    "* Regresión lineal: la variable independiente a predecir es del tipo **continuo**\n",
    "  * El peso y la altura de una persona\n",
    "  * salario de una persona\n",
    "  * distancia de un recorrido \n",
    "  * ...\n",
    "  \n",
    "* Regresion logística: mientras este predice variables **nominales**\n",
    "  * Frutas y verduras\n",
    "  * region\n",
    "  * grupo sanguineo\n",
    "  * ...\n",
    "\n",
    "Es decir tanto el algoritmo de regresión como el de clasificación surgieron para adecuar a las caracteristas de estos tipos de variable, para poder predecirlos con alta precision y exhaustividad."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, existe todavía un tipo de variable que es muy comun, se trata de una variable **ordinal**, por ejemplo, el nivel de satifaccion de las encuestas, la nota descriptica de la evaluacion del trimestre, puestos conseguidos en los deportes competitivos...\n",
    "\n",
    "Como se puede apreciar es como una mezcla de los dos tipos de variables mencionados anteriormente, donde no puede tomar cualquier valor entre dos valores consecutivos pero mantiene cierta relacion de continuidad, el motivo por el cual tampoco puede considerarse como variable nominal, ya que los valores no son independientes entre ellos, es decir, es erronea la prediccion de una variable ordinal si tuviera que elegir entre *Muy satisfecho* y *Muy insatisfecho*.\n",
    "\n",
    "<img src=\"./images/satisfaccion.jpg\" alt=\"satisfaccion.jpg\" width=\"1000\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A continuación vamos a ver varios metodos regresion y clasificacion los cuales podemos obtener una aproximacion a la solucion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización\n",
    "\n",
    "Inicilización de pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero lo descargamos en local\n",
    "import pyspark                                 # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import urllib\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones útiles \n",
    "\n",
    "A continuación se proporcionan algunas de las funciones que pueden ser útiles, por ejemplo, las funciones para la evaluación de los modelos creados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from sklearn.metrics import  ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pyspark.sql.functions as f \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_spark_dataframe(file):\n",
    "    \"\"\"crea dataframe spark a partir de un fichero\n",
    "\n",
    "    Args:\n",
    "        file (path): ruta del fichero\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').load(file)\n",
    "    return df\n",
    "\n",
    "def accuracy(predictions, label, prediction=\"prediction\"):\n",
    "    \"\"\"calcular la exhaustitud de la prediccion\n",
    "\n",
    "    Args:\n",
    "        predictions (): predicciones obtenidas\n",
    "        label (col): columna de los valores reales \n",
    "        prediction (str, optional): columna de la prediccion. Defaults to \"prediction\".\n",
    "    \"\"\"\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label, predictionCol = prediction, metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Classification accuracy = %g\" % (accuracy))\n",
    "\n",
    "def f1(predictions, label, prediction=\"prediction\"):\n",
    "    \"\"\"calcular la F-Measure de la prediccion\n",
    "\n",
    "    Args:\n",
    "        predictions (_type_):  predicciones obtenidas\n",
    "        label (col): columna de los valores reales \n",
    "        prediction (str, optional): columna de la prediccion. Defaults to \"prediction\".\n",
    "    \"\"\"\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label, predictionCol=prediction, metricName=\"f1\")\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "    print(\"Classification f1 score= \" + str(f1))\n",
    "\n",
    "def calcular_matriz_confusion(predictions,label):\n",
    "    \"\"\"Mostrar la matriz de confusion\n",
    "\n",
    "    Args:\n",
    "        predictions (_type_):  predicciones obtenidas\n",
    "        label (col): columna de los valores reales \n",
    "    \"\"\"\n",
    "    preds_and_labels = predictions.select(['prediction', label])\n",
    "    # hay que convertir las columnas prediction label en una tupla\n",
    "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "    cm = metrics.confusionMatrix()\n",
    "    \n",
    "    # para mostrar las etiquetas...muy lioso para poca cosa\n",
    "    class_temp = predictions.select(label).groupBy(label)\\\n",
    "                            .count().sort('count', ascending=False).toPandas()\n",
    "    class_names = class_temp[label].values.tolist()\n",
    "    cm = cm.toArray().astype(np.int64)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    ConfusionMatrixDisplay(cm, display_labels=class_names).plot(ax =ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A continuación se describe cada una de las columnas del dataframe:\n",
    "\n",
    "* `Sales`: Ventas unitarias de cierto producto (en miles) en cada ubicación\n",
    "* `CompPrice`: Precio cobrado en cada ubicación\n",
    "* `Income`:Nivel de ingresos \n",
    "* `Advertising`:  Presupuesto de publicidad local para la empresa en cada ubicación\n",
    "* `Population`: población en la región\n",
    "* `Price`: Costes\n",
    "* `Age`: Edad media de la población\n",
    "* `Education`: Nivel de educación en cada lugar\n",
    "* `Urban`: Un factor con niveles ``No`` y ``Sí`` para indicar si la tienda\n",
    "   está en una zona urbana o rural\n",
    "* `ShelveLoc`: Un factor con niveles ``Malo``, ``Bueno`` y ``Medio`` que indica el\n",
    "   calidad de la ubicación\n",
    "* `US`: Un factor con niveles ``No`` y ``Sí`` para indicar si la tienda\n",
    "   está en los Estados Unidos o no\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_spark_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./carkids.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39m# ahora llamamos a la función\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[39m=\u001b[39m create_spark_dataframe(file)\n\u001b[1;32m      4\u001b[0m df\u001b[39m.\u001b[39mshow(\u001b[39m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_spark_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "file = \"./carkids.csv\"\n",
    "# ahora llamamos a la función\n",
    "df = create_spark_dataframe(file)\n",
    "df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesado de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que realizar un pre-procesado de las variables ordinales y nominales:\n",
    "\n",
    "* `Sales`       => `volumen_ventas`: se divide en tres intervalos considerando como `alto`, `medio`, `bajo`\n",
    "* `ShelveLoc`   => `ShelveLocIndex`: convertir ``Malo``, ``Bueno`` y ``Medio`` en numeros consecutivos enteros\n",
    "* `Urban`       => `UrbanIndex`: procesado de variable nominal `0` y `1`\n",
    "* `US`          => `USIndex`: procesado de variable nominal `0` y `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "# procesado de la variable ordinal\n",
    "_df = df.withColumn('ShelveLocIndex', when(df.ShelveLoc == 'Bad', 0.0).when(df.ShelveLoc == 'Medium', 1.0).otherwise(2.0))\n",
    "# arbitrariamente se divide el volumen de ventas en tres categorias\n",
    "_df = _df.withColumn('volumen_ventas', when(df.Sales<5.5, 0.0).when(df.Sales<9, 1.0).otherwise(2.0))\n",
    "_df.show(5)\n",
    "\n",
    "# procesado de las variables nominales mediante StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Urban\", outputCol=\"UrbanIndex\")\n",
    "indexed_model = indexer.fit(_df)\n",
    "_df = indexed_model.transform(_df)\n",
    "indexer = StringIndexer(inputCol=\"US\", outputCol=\"USIndex\")\n",
    "indexed_model = indexer.fit(_df)\n",
    "_df = indexed_model.transform(_df)\n",
    "\n",
    "# descartar las columnas originales\n",
    "_df = _df.drop('Urban').drop('US').drop('Sales').drop('ShelveLoc')\n",
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# crear una columna feature que consiste en una lista de todas las variables dependientes\n",
    "cols = _df.columns\n",
    "cols.remove('volumen_ventas')\n",
    "assembler = VectorAssembler(inputCols = cols, outputCol ='features')\n",
    "df_features = assembler.transform(_df)\n",
    "df_features.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train y Test\n",
    "\n",
    "División entre entrenamiento y test.\n",
    "* 70% de valores para entrenar \n",
    "* 30% para test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sol:\n",
    "df_features = df_features.select(['volumen_ventas', 'features'])\n",
    "train, test = df_features.randomSplit([0.7, 0.3], seed = 1234)\n",
    "df_features.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clasificacion mediante Regresion Logística"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Se utilizada la distribucion multinomial para clasificar**\n",
    "2. **Mediante un validacion cruzada se obtiene los mejores hiperparametros**\n",
    "3. **Se evalua cada modelo generado utilizando la metrica F-Measure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# regresion logistica utilizando la distribucion multinomial\n",
    "mlr = LogisticRegression(labelCol=\"volumen_ventas\", family=\"multinomial\")\n",
    "\n",
    "# conjunto de combinaciones de parametros\n",
    "paramGrid_mlr = ParamGridBuilder().baseOn([mlr.labelCol, 'volumen_ventas'])\\\n",
    "             .baseOn([mlr.family, 'multinomial'])  \\\n",
    "             .addGrid(mlr.regParam, [0.01, 0.1, 0.5, 1.0, 2.0]) \\\n",
    "            .addGrid(mlr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "             .build()\n",
    "\n",
    "# evaluador del modelo\n",
    "cvEvaluator= MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='volumen_ventas')\n",
    "\n",
    "# validacion cruzada\n",
    "cv_mlr = CrossValidator(estimator=mlr, estimatorParamMaps=paramGrid_mlr, evaluator=cvEvaluator)\n",
    "cvModel_mlr = cv_mlr.fit(train)\n",
    "\n",
    "def printBestParam(cvModel):\n",
    "    \"\"\"Printea los parametros del mejor modelo\n",
    "\n",
    "    Args:\n",
    "        cvModel (model): modelo del crossValidator\n",
    "    \"\"\"\n",
    "    bestModel = cvModel.bestModel\n",
    "    bestParams = bestModel.extractParamMap()\n",
    "    for key in bestParams.keys():\n",
    "        print(key,': ', bestParams[key])\n",
    "\n",
    "printBestParam(cvModel_mlr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construye la regresion logistica con los mejores parametros y muestra los detalles del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_best = LogisticRegression(labelCol=\"volumen_ventas\", family=\"multinomial\", regParam=0.01, elasticNetParam=0.1)\n",
    "mlrModel_best = mlr_best.fit(train)\n",
    "\n",
    "print(\"Coefficients: \\n\" + str(mlrModel_best.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(mlrModel_best.interceptVector))\n",
    "\n",
    "trainingSummary = mlrModel_best.summary\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 y la matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mlr = mlrModel_best.transform(test)\n",
    "f1(predictions_mlr, 'volumen_ventas')\n",
    "calcular_matriz_confusion(predictions_mlr, 'volumen_ventas')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clasificacion mediante OneVsRest\n",
    "\n",
    "OneVsRest es un ejemplo de  aprendizaje automático para realizar una clasificación multiclase dado un clasificador base a partir del cual realiza una clasificación binaria de forma eficiente. También se conoce como \"One-vs-All\".\n",
    "\n",
    "OneVsRest se implementa como un estimador. Para el clasificador base, toma instancias del dicho clasificador y crea un problema de clasificación binaria para cada una de las k clases. El clasificador para la clase i se entrena para predecir si la etiqueta es i o no, distinguiendo la clase i del resto de clases.\n",
    "\n",
    "Las predicciones se realizan evaluando cada clasificador binario y el índice del clasificador más fiable se utiliza como etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "lr = LogisticRegression(labelCol=\"volumen_ventas\", family=\"binomial\")\n",
    "\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(labelCol=\"volumen_ventas\", classifier=lr)\n",
    "\n",
    "# train the multiclass model.\n",
    "ovrModel = ovr.fit(train)\n",
    "\n",
    "# score the model on test data.\n",
    "predictions_ovr = ovrModel.transform(test)\n",
    "\n",
    "f1(predictions_ovr, \"volumen_ventas\")\n",
    "calcular_matriz_confusion(predictions_mlr, 'volumen_ventas')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clasificacion mediante Regresion Lineal\n",
    "\n",
    "Se utilizar el modelo GeneralizedLinearReggression configurado:\n",
    "* Distribucion binomial\n",
    "* Funcion de enlace `logit` y `probit` (La función de enlace proporciona la relación entre el predictor lineal y la media de la función de distribución.)\n",
    "\n",
    "#### Función logit y probit\n",
    "![Funcion logit](./images/Logit-vs-probit-models.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procesado de datos\n",
    "\n",
    "![cumulative model](./images/cumulative.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separar volumen_ventas en dos cojuntos {0}, {1,2}\n",
    "df_features_l = df_features.withColumn('volumen_ventas_1', when(df_features.volumen_ventas == 0.0, 0.0).otherwise(1.0))\n",
    "# separar volumen_ventas en dos cojuntos {0,1}, {2}\n",
    "df_features_l = df_features_l.withColumn('volumen_ventas_2', when(df_features_l.volumen_ventas <= 1.0, 0.0).otherwise(1.0))\n",
    "df_features_l = df_features_l.select(['volumen_ventas', 'volumen_ventas_1','volumen_ventas_2', 'features'])\n",
    "train_l, test_l = df_features_l.randomSplit([0.7, 0.3], seed = 1234)\n",
    "df_features_l.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construye la regresion linear con los mejores parametros y muestra los detalles del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "glr = GeneralizedLinearRegression(labelCol=\"volumen_ventas_1\")\n",
    "\n",
    "paramGrid_glr = ParamGridBuilder().baseOn([glr.labelCol, 'volumen_ventas_1'])\\\n",
    "             .baseOn([glr.predictionCol, 'prediction_1'])\\\n",
    "             .baseOn([glr.family, 'binomial'])  \\\n",
    "             .addGrid(glr.link, [\"logit\", \"probit\"]) \\\n",
    "             .addGrid(glr.regParam, [0.01, 0.1, 0.5, 1.0, 2.0]) \\\n",
    "             .build()\n",
    "\n",
    "cvEvaluator= MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='volumen_ventas_1', predictionCol=\"prediction_1\")\n",
    "\n",
    "cv_glr = CrossValidator(estimator=glr, estimatorParamMaps=paramGrid_glr, evaluator=cvEvaluator)\n",
    "cvModel_glr = cv_glr.fit(train_l)\n",
    "\n",
    "printBestParam(cvModel_glr)\n",
    "\n",
    "glr_2 = GeneralizedLinearRegression(labelCol=\"volumen_ventas_2\")\n",
    "\n",
    "paramGrid_glr_2 = ParamGridBuilder().baseOn([glr_2.labelCol, 'volumen_ventas_2'])\\\n",
    "            .baseOn([glr_2.predictionCol, 'prediction_2'])\\\n",
    "             .baseOn([glr_2.family, 'binomial'])  \\\n",
    "             .addGrid(glr_2.link, [\"logit\", \"probit\"]) \\\n",
    "             .addGrid(glr_2.regParam, [0.01, 0.1, 0.5, 1.0, 2.0]) \\\n",
    "             .build()\n",
    "\n",
    "cvEvaluator= MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='volumen_ventas_2', predictionCol=\"prediction_2\")\n",
    "\n",
    "cv_glr_2 = CrossValidator(estimator=glr_2, estimatorParamMaps=paramGrid_glr_2, evaluator=cvEvaluator)\n",
    "cvModel_glr_2 = cv_glr_2.fit(train_l)\n",
    "\n",
    "printBestParam(cvModel_glr_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinar ambas predicciones haciendo disyuncion para poder separar las predicciones en la misma cantidad de variable ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1 = cvModel_glr.transform(test_l)\n",
    "test_ll = predictions_1.select(['volumen_ventas', 'volumen_ventas_1', 'features', 'prediction_1'])\n",
    "predictions_2 = cvModel_glr_2.transform(test_ll)\n",
    "predictions_2.show(5, False)\n",
    "df_predictions = predictions_2.withColumn('prediction', when(predictions_2.prediction_1 < 0.45, 0.0).when(predictions_2.prediction_2 >= 0.5, 2.0).otherwise(1.0))\n",
    "df_predictions.show(5, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mostrar las metricas obtenidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(df_predictions, \"volumen_ventas\")\n",
    "calcular_matriz_confusion(df_predictions, \"volumen_ventas\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "TDM venv Kernel",
   "language": "python",
   "name": "tdmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
