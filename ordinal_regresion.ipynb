{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal Ordinal\n",
    "\n",
    "### Xukai Chen & Julio \n",
    "### Curso: TDM - 2022/2023\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para  poder entender el concepto esta regresión y el problema a resolver, debemos recordar y recurrir a los conceptos basicos del data science, las variables.\n",
    "\n",
    "Como ya sabemos, tanto regresión como clasificación, son un modelo de estimacion que tienen como objetivo predecir una relacion de dependencia entre una variable independiente  *Y* y una o varias variables dependientes  *X*.\n",
    "\n",
    "<img src=\"./images/regr_cfc.png\" alt=\"regre_cfc.png\" width=\"600\"/>\n",
    "\n",
    "#### Los ejemplos más destacados de ambos tipos de algoritmos \n",
    "* Regresión lineal: la variable independiente a predecir es del tipo **continuo**\n",
    "  * El peso y la altura de una persona\n",
    "  * salario de una persona\n",
    "  * distancia de un recorrido \n",
    "  * ...\n",
    "  \n",
    "* Regresion logística: mientras este predice variables **nominales**\n",
    "  * Frutas y verduras\n",
    "  * region\n",
    "  * grupo sanguineo\n",
    "  * ...\n",
    "\n",
    "Es decir tanto el algoritmo de regresión como el de clasificación surgieron para adecuar a las caracteristas de estos tipos de variable, para poder predecirlos con alta precision y exhaustividad."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, existe todavía un tipo de variable que es muy comun, se trata de una variable **ordinal**, por ejemplo, el nivel de satifaccion de las encuestas, la nota descriptica de la evaluacion del trimestre, puestos conseguidos en los deportes competitivos...\n",
    "\n",
    "Como se puede apreciar es como una mezcla de los dos tipos de variables mencionados anteriormente, donde no puede tomar cualquier valor entre dos valores consecutivos pero mantiene cierta relacion de continuidad, el motivo por el cual tampoco puede considerarse como variable nominal, ya que los valores no son independientes entre ellos, es decir, es erronea la prediccion de una variable ordinal si tuviera que elegir entre *Muy satisfecho* y *Muy insatisfecho*.\n",
    "\n",
    "<img src=\"./images/satisfaccion.jpg\" alt=\"satisfaccion.jpg\" width=\"1000\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A continuación vamos a ver varios metodos regresion y clasificacion los cuales podemos obtener una aproximacion a la solucion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización\n",
    "\n",
    "Inicilización de pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# primero lo descargamos en local\n",
    "import pyspark                                 # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import urllib\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones útiles \n",
    "\n",
    "A continuación se proporcionan algunas de las funciones que pueden ser útiles, por ejemplo, las funciones para la evaluación de los modelos creados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from sklearn.metrics import  ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pyspark.sql.functions as f \n",
    "import numpy as np\n",
    "\n",
    "########\n",
    "# Creación del dataframe a partir de un ficheroE\n",
    "########\n",
    "def create_spark_dataframe(file):\n",
    "    df = spark.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').load(file)\n",
    "    return df\n",
    "\n",
    "def accuracy(predictions, label, prediction=\"prediction\"):    \n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label, predictionCol = prediction, metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Classification accuracy = %g\" % (accuracy))\n",
    "    return accuracy\n",
    "\n",
    "def f1(predictions, label, prediction=\"prediction\"):\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label, predictionCol=prediction, metricName=\"f1\")\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "    print(\"Classification f1 score= \" + str(f1))\n",
    "    return f1\n",
    "\n",
    "def calcular_matriz_confusion(predictions,label):      \n",
    "    preds_and_labels = predictions.select(['prediction', label])\n",
    "    # hay que convertir las columnas prediction label en una tupla\n",
    "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "    cm = metrics.confusionMatrix()\n",
    "    \n",
    "    # para mostrar las etiquetas...muy lioso para poca cosa\n",
    "    class_temp = predictions.select(label).groupBy(label)\\\n",
    "                            .count().sort('count', ascending=False).toPandas()\n",
    "    class_names = class_temp[label].values.tolist()\n",
    "    cm = cm.toArray().astype(np.int64)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    ConfusionMatrixDisplay(cm, display_labels=class_names).plot(ax =ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "file = \"./carkids.csv\"\n",
    "# ahora llamamos a la función\n",
    "df = create_spark_dataframe(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A continuación se describe cada una de las columnas del dataframe:\n",
    "\n",
    "* `Sales`: Ventas unitarias de cierto producto (en miles) en cada ubicación\n",
    "* `CompPrice`: Precio cobrado en cada ubicación\n",
    "* `Income`:Nivel de ingresos \n",
    "* `Advertising`:  Presupuesto de publicidad local para la empresa en cada ubicación\n",
    "* `Population`: población en la región\n",
    "* `Price`: Costes\n",
    "* `Age`: Edad media de la población\n",
    "* `Education`: Nivel de educación en cada lugar\n",
    "* `Urban`: Un factor con niveles ``No`` y ``Sí`` para indicar si la tienda\n",
    "   está en una zona urbana o rural\n",
    "* `ShelveLoc`: Un factor con niveles ``Malo``, ``Bueno`` y ``Medio`` que indica el\n",
    "   calidad de la ubicación\n",
    "* `US`: Un factor con niveles ``No`` y ``Sí`` para indicar si la tienda\n",
    "   está en los Estados Unidos o no\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "_df = df.withColumn('ShelveLocIndex', when(df.ShelveLoc == 'Bad', 0.0).when(df.ShelveLoc == 'Medium', 1.0).otherwise(2.0))\n",
    "_df = _df.withColumn('volumen_ventas', when(df.Sales<5.5, 0.0).when(df.Sales<9, 1.0).otherwise(2.0))\n",
    "_df.show(5)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Urban\", outputCol=\"UrbanIndex\")\n",
    "indexed_model = indexer.fit(_df)\n",
    "_df = indexed_model.transform(_df)\n",
    "indexer = StringIndexer(inputCol=\"US\", outputCol=\"USIndex\")\n",
    "indexed_model = indexer.fit(_df)\n",
    "_df = indexed_model.transform(_df)\n",
    "_df = _df.drop('Urban').drop('US').drop('Sales').drop('ShelveLoc')\n",
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6\n",
    "\n",
    "Comenzamos con la preparación de los datos para la construcción del modelo. El objetivo es predecir el valor de la columna `volumen_ventas` a partir del resto de columnas.\n",
    "\n",
    "Compararemos 2 modelos: Decision trees y  Naïve Bayes\n",
    "\n",
    "\n",
    "Combinar en un `VectorAssembler` todas las columnas excepto la columna `volumen_ventas`, dando como salida una nueva columna `features`. Este transformador tomará el dataframe resultado del apartado anterior como entrada. El dataframe de salida se debe llamar `df_features`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sol:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "cols = _df.columns\n",
    "cols.remove('volumen_ventas')\n",
    "assembler = VectorAssembler( inputCols = cols,\n",
    "                             outputCol ='features')\n",
    "df_features = assembler.transform(_df)\n",
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 7. \n",
    "\n",
    "División entre entrenamiento y test.\n",
    "* 70% de valores para entrenar \n",
    "* 30% para test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sol:\n",
    "df_features = df_features.select(['volumen_ventas', 'features'])\n",
    "train, test = df_features.randomSplit([0.7, 0.3], seed = 1234)\n",
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "mlr = LogisticRegression(labelCol=\"volumen_ventas\", family=\"multinomial\")\n",
    "\n",
    "paramGrid_mlr = ParamGridBuilder().baseOn([mlr.labelCol, 'volumen_ventas'])\\\n",
    "             .baseOn([mlr.family, 'multinomial'])  \\\n",
    "             .addGrid(mlr.regParam, [0.01, 0.1, 0.5, 1.0, 2.0]) \\\n",
    "            .addGrid(mlr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "             .build()\n",
    "\n",
    "cvEvaluator= MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='volumen_ventas')\n",
    "\n",
    "cv_mlr = CrossValidator(estimator=mlr, estimatorParamMaps=paramGrid_mlr, evaluator=cvEvaluator)\n",
    "cvModel_mlr = cv_mlr.fit(train)\n",
    "\n",
    "def printBestParam(cvModel):\n",
    "    bestModel = cvModel.bestModel\n",
    "\n",
    "    bestParams = bestModel.extractParamMap()\n",
    "    for key in bestParams.keys():\n",
    "        print(key,': ', bestParams[key])\n",
    "\n",
    "printBestParam(cvModel_mlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mlr_best = LogisticRegression(labelCol=\"volumen_ventas\", family=\"multinomial\", regParam=0.01, elasticNetParam=0.1)\n",
    "mlrModel_best = mlr_best.fit(train)\n",
    "\n",
    "print(\"Coefficients: \\n\" + str(mlrModel_best.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(mlrModel_best.interceptVector))\n",
    "\n",
    "trainingSummary = mlrModel_best.summary\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "predictions_mlr = mlrModel_best.transform(test)\n",
    "f1(predictions_mlr, 'volumen_ventas')\n",
    "calcular_matriz_confusion(predictions_mlr, 'volumen_ventas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "lr = LogisticRegression(labelCol=\"volumen_ventas\", family=\"binomial\")\n",
    "\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(labelCol=\"volumen_ventas\", classifier=lr)\n",
    "\n",
    "# train the multiclass model.\n",
    "ovrModel = ovr.fit(train)\n",
    "\n",
    "# score the model on test data.\n",
    "predictions_ovr = ovrModel.transform(test)\n",
    "\n",
    "f1(predictions_ovr, \"volumen_ventas\")\n",
    "calcular_matriz_confusion(predictions_mlr, 'volumen_ventas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_features_l = df_features.withColumn('volumen_ventas_1', when(df_features.volumen_ventas == 0.0, 0.0).otherwise(1.0))\n",
    "df_features_l = df_features_l.withColumn('volumen_ventas_2', when(df_features_l.volumen_ventas <= 1.0, 0.0).otherwise(1.0))\n",
    "df_features_l = df_features_l.select(['volumen_ventas', 'volumen_ventas_1','volumen_ventas_2', 'features'])\n",
    "train_l, test_l = df_features_l.randomSplit([0.7, 0.3], seed = 1234)\n",
    "df_features_l.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "glr = GeneralizedLinearRegression(labelCol=\"volumen_ventas_1\")\n",
    "\n",
    "paramGrid_glr = ParamGridBuilder().baseOn([glr.labelCol, 'volumen_ventas_1'])\\\n",
    "             .baseOn([glr.predictionCol, 'prediction_1'])\\\n",
    "             .baseOn([glr.family, 'binomial'])  \\\n",
    "             .addGrid(glr.link, [\"logit\", \"probit\"]) \\\n",
    "             .addGrid(glr.regParam, [0.01, 0.1, 0.5, 1.0, 2.0]) \\\n",
    "             .build()\n",
    "\n",
    "cvEvaluator= MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='volumen_ventas_1', predictionCol=\"prediction_1\")\n",
    "\n",
    "cv_glr = CrossValidator(estimator=glr, estimatorParamMaps=paramGrid_glr, evaluator=cvEvaluator)\n",
    "cvModel_glr = cv_glr.fit(train_l)\n",
    "\n",
    "printBestParam(cvModel_glr)\n",
    "\n",
    "glr_2 = GeneralizedLinearRegression(labelCol=\"volumen_ventas_2\")\n",
    "\n",
    "paramGrid_glr_2 = ParamGridBuilder().baseOn([glr_2.labelCol, 'volumen_ventas_2'])\\\n",
    "            .baseOn([glr_2.predictionCol, 'prediction_2'])\\\n",
    "             .baseOn([glr_2.family, 'binomial'])  \\\n",
    "             .addGrid(glr_2.link, [\"logit\", \"probit\"]) \\\n",
    "             .addGrid(glr_2.regParam, [0.01, 0.1, 0.5, 1.0, 2.0]) \\\n",
    "             .build()\n",
    "\n",
    "cvEvaluator= MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='volumen_ventas_2', predictionCol=\"prediction_2\")\n",
    "\n",
    "cv_glr_2 = CrossValidator(estimator=glr_2, estimatorParamMaps=paramGrid_glr_2, evaluator=cvEvaluator)\n",
    "cvModel_glr_2 = cv_glr_2.fit(train_l)\n",
    "\n",
    "printBestParam(cvModel_glr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "predictions_1 = cvModel_glr.transform(test_l)\n",
    "test_ll = predictions_1.select(['volumen_ventas', 'volumen_ventas_1', 'features', 'prediction_1'])\n",
    "predictions_2 = cvModel_glr_2.transform(test_ll)\n",
    "predictions_2.show(5, False)\n",
    "df_predictions = predictions_2.withColumn('prediction', when(predictions_2.prediction_1 < 0.45, 0.0).when(predictions_2.prediction_2 >= 0.5, 2.0).otherwise(1.0))\n",
    "df_predictions.show(5, False)\n",
    "f1(df_predictions, \"volumen_ventas\")\n",
    "calcular_matriz_confusion(df_predictions, \"volumen_ventas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lr_1 = LogisticRegression(labelCol=\"volumen_ventas_1\", family=\"binomial\", predictionCol=\"prediction_1\")\n",
    "lr_1_Model = lr_1.fit(train_l)\n",
    "predictions_1 = lr_1_Model.transform(test_l)\n",
    "lr_2 = LogisticRegression(labelCol=\"volumen_ventas_2\", family=\"binomial\", predictionCol=\"prediction_2\")\n",
    "lr_2_Model = lr_2.fit(train_l)\n",
    "test_ll = predictions_1.select(['volumen_ventas', 'volumen_ventas_1', 'volumen_ventas_1', 'features', 'prediction_1'])\n",
    "predictions_2 = lr_2_Model.transform(test_ll)\n",
    "predictions_2.show(5, False)\n",
    "df_predictions = predictions_2.withColumn('prediction', when(predictions_2.prediction_1 == 0.0, 0.0).when(predictions_2.prediction_2 == 1.0, 2.0).otherwise(1.0))\n",
    "df_predictions.show(5, False)\n",
    "f1(df_predictions, \"volumen_ventas\")\n",
    "calcular_matriz_confusion(df_predictions, \"volumen_ventas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "TDM venv Kernel",
   "language": "python",
   "name": "tdmenv"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
